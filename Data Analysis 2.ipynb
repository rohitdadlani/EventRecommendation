{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "509bb296",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Dask dataframe requirements are not installed.\n\nPlease either conda or pip install as follows:\n\n  conda install dask                     # either conda install\n  python -m pip install \"dask[dataframe]\" --upgrade  # or python -m pip install",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\dask\\dataframe\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataframe\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackends\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethods\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrolling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_testing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtest_dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\dask\\dataframe\\backends.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCreationDispatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDaskBackendEntrypoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPANDAS_GE_220\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_any_real_numeric_dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mScalar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'PANDAS_GE_220' from 'dask.dataframe._compat' (C:\\Users\\rohit\\anaconda3\\lib\\site-packages\\dask\\dataframe\\_compat.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32836/1887692589.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataframe\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Load data using Dask, specifying data types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\dask\\dataframe\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;34m'  python -m pip install \"dask[dataframe]\" --upgrade  # or python -m pip install'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     )\n\u001b[1;32m--> 122\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Dask dataframe requirements are not installed.\n\nPlease either conda or pip install as follows:\n\n  conda install dask                     # either conda install\n  python -m pip install \"dask[dataframe]\" --upgrade  # or python -m pip install"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Load data using Dask, specifying data types\n",
    "train = dd.read_csv('train.csv')\n",
    "test = dd.read_csv('test.csv')\n",
    "users = dd.read_csv('users.csv', dtype={'birthyear': 'object', 'timezone': 'float64'})\n",
    "user_friends = dd.read_csv('user_friends.csv.gz')\n",
    "events = dd.read_csv('events.csv.gz', dtype={'city': 'object', 'country': 'object', 'state': 'object', 'zip': 'object'})\n",
    "event_attendees = dd.read_csv('event_attendees.csv.gz')\n",
    "\n",
    "# Preview the data\n",
    "print(\"Train.csv:\")\n",
    "print(train.head(), \"\\n\")\n",
    "\n",
    "print(\"Test.csv:\")\n",
    "print(test.head(), \"\\n\")\n",
    "\n",
    "print(\"Users.csv:\")\n",
    "print(users.head(), \"\\n\")\n",
    "\n",
    "print(\"User_friends.csv:\")\n",
    "print(user_friends.head(), \"\\n\")\n",
    "\n",
    "print(\"Events.csv:\")\n",
    "print(events.head(), \"\\n\")\n",
    "\n",
    "print(\"Event_attendees.csv:\")\n",
    "print(event_attendees.head(), \"\\n\")\n",
    "\n",
    "# Data preprocessing\n",
    "train['timestamp'] = dd.to_datetime(train['timestamp'], errors='coerce')\n",
    "test['timestamp'] = dd.to_datetime(test['timestamp'], errors='coerce')\n",
    "users['joinedAt'] = dd.to_datetime(users['joinedAt'], errors='coerce')\n",
    "events['start_time'] = dd.to_datetime(events['start_time'], errors='coerce')\n",
    "\n",
    "# Convert Dask data to Pandas DataFrame for EDA and further analysis\n",
    "train_pd = train.compute()\n",
    "test_pd = test.compute()\n",
    "users_pd = users.compute()\n",
    "user_friends_pd = user_friends.compute()\n",
    "events_pd = events.compute()\n",
    "event_attendees_pd = event_attendees.compute()\n",
    "\n",
    "# EDA Analysis\n",
    "print(\"Train Data Info\")\n",
    "print(train_pd.info())\n",
    "\n",
    "print(\"\\nTest Data Info\")\n",
    "print(test_pd.info())\n",
    "\n",
    "print(\"\\nUsers Data Info\")\n",
    "print(users_pd.info())\n",
    "\n",
    "print(\"\\nUser Friends Data Info\")\n",
    "print(user_friends_pd.info())\n",
    "\n",
    "print(\"\\nEvents Data Info\")\n",
    "print(events_pd.info())\n",
    "\n",
    "print(\"\\nEvent Attendees Data Info\")\n",
    "print(event_attendees_pd.info())\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nTrain Data Statistics\")\n",
    "print(train_pd.describe())\n",
    "\n",
    "print(\"\\nTest Data Statistics\")\n",
    "print(test_pd.describe())\n",
    "\n",
    "print(\"\\nUsers Data Statistics\")\n",
    "print(users_pd.describe())\n",
    "\n",
    "print(\"\\nEvents Data Statistics\")\n",
    "print(events_pd.describe())\n",
    "\n",
    "# Visualizing missing values\n",
    "def plot_missing_values(df, title):\n",
    "    missing = df.isnull().mean()\n",
    "    missing = missing[missing > 0]\n",
    "    missing.sort_values(inplace=True)\n",
    "    missing.plot.bar()\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_missing_values(train_pd, \"Missing Values in Train Data\")\n",
    "plot_missing_values(test_pd, \"Missing Values in Test Data\")\n",
    "plot_missing_values(users_pd, \"Missing Values in Users Data\")\n",
    "plot_missing_values(events_pd, \"Missing Values in Events Data\")\n",
    "\n",
    "# Distribution of interested and not_interested in train data\n",
    "train_pd['interested'].value_counts().plot(kind='bar', title='Distribution of Interested and Not Interested')\n",
    "plt.show()\n",
    "\n",
    "# Creating user and event mappings\n",
    "user_ids = users_pd['user_id'].unique().tolist()\n",
    "event_ids = train_pd['event'].unique().tolist()\n",
    "\n",
    "user_to_index = {x: i for i, x in enumerate(user_ids)}\n",
    "event_to_index = {x: i for i, x in enumerate(event_ids)}\n",
    "\n",
    "train_pd['user'] = train_pd['user'].map(user_to_index)\n",
    "train_pd['event'] = train_pd['event'].map(event_to_index)\n",
    "test_pd['user'] = test_pd['user'].map(user_to_index)\n",
    "test_pd['event'] = test_pd['event'].map(event_to_index)\n",
    "\n",
    "# Checking for valid users and events in the test set\n",
    "test_users = test_pd['user'].unique()\n",
    "test_events = test_pd['event'].unique()\n",
    "\n",
    "valid_users = np.isin(test_users, list(user_to_index.values()))\n",
    "valid_events = np.isin(test_events, list(event_to_index.values()))\n",
    "\n",
    "print(f\"Number of users in the test set: {len(test_users)}\")\n",
    "print(f\"Number of events in the test set: {len(test_events)}\")\n",
    "print(f\"Number of users in the test set also in the training set: {np.sum(valid_users)}\")\n",
    "print(f\"Number of events in the test set also in the training set: {np.sum(valid_events)}\")\n",
    "\n",
    "# Filter the test data\n",
    "test_pd = test_pd[test_pd['user'].isin(user_to_index.values()) & test_pd['event'].isin(event_to_index.values())]\n",
    "\n",
    "# Check that the test set is not empty after filtering\n",
    "if test_pd.empty:\n",
    "    print(\"The test set is empty after filtering. Ensure data is correct.\")\n",
    "else:\n",
    "    num_users = len(user_ids)\n",
    "    num_events = len(event_ids)\n",
    "\n",
    "    # Prepare data for the model\n",
    "    X = train_pd[['user', 'event']].values\n",
    "    y = train_pd['interested'].values\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create matrix factorization model\n",
    "    class RecommenderNet(tf.keras.Model):\n",
    "        def __init__(self, num_users, num_events, embedding_size=50, **kwargs):\n",
    "            super(RecommenderNet, self).__init__(**kwargs)\n",
    "            self.user_embedding = tf.keras.layers.Embedding(\n",
    "                num_users, embedding_size,\n",
    "                embeddings_initializer='he_normal',\n",
    "                embeddings_regularizer=tf.keras.regularizers.l2(1e-6)\n",
    "            )\n",
    "            self.event_embedding = tf.keras.layers.Embedding(\n",
    "                num_events, embedding_size,\n",
    "                embeddings_initializer='he_normal',\n",
    "                embeddings_regularizer=tf.keras.regularizers.l2(1e-6)\n",
    "            )\n",
    "            self.user_bias = tf.keras.layers.Embedding(num_users, 1)\n",
    "            self.event_bias = tf.keras.layers.Embedding(num_events, 1)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            user_vector = self.user_embedding(inputs[:, 0])\n",
    "            event_vector = self.event_embedding(inputs[:, 1])\n",
    "            \n",
    "            user_bias = self.user_bias(inputs[:, 0])\n",
    "            event_bias = self.event_bias(inputs[:, 1])\n",
    "            \n",
    "            dot_user_event = tf.tensordot(user_vector, event_vector, 2)\n",
    "            \n",
    "            x = dot_user_event + user_bias + event_bias\n",
    "            \n",
    "            return tf.nn.sigmoid(x)\n",
    "\n",
    "    # Model parameters\n",
    "    embedding_size = 50\n",
    "\n",
    "    model = RecommenderNet(num_users, num_events, embedding_size)\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        metrics=[tf.keras.metrics.AUC()]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        x=X_train,\n",
    "        y=y_train,\n",
    "        batch_size=64,\n",
    "        epochs=10,\n",
    "        validation_data=(X_val, y_val)\n",
    "    )\n",
    "\n",
    "    # Function for making recommendations\n",
    "    def recommend(user_id, model, num_recommendations=10):\n",
    "        if user_id not in user_to_index:\n",
    "            print(f\"User ID {user_id} not found.\")\n",
    "            return []\n",
    "            \n",
    "        user_index = user_to_index[user_id]\n",
    "        event_indices = np.arange(num_events)\n",
    "        \n",
    "        user_array = np.array([user_index] * num_events)\n",
    "        event_array = event_indices\n",
    "        \n",
    "        predictions = model.predict(np.vstack([user_array, event_array]).T).flatten()\n",
    "        \n",
    "        top_indices = predictions.argsort()[-num_recommendations:][::-1]\n",
    "        recommended_events = [event_ids[i] for i in top_indices]\n",
    "        \n",
    "        return recommended_events\n",
    "\n",
    "    # Create the submission file\n",
    "    submission = []\n",
    "\n",
    "    for user_id in test_pd['user'].unique():\n",
    "        user_id = int(user_id)  # Convert to integer\n",
    "        original_user_id = user_ids[user_id]\n",
    "        recommendations = recommend(original_user_id, model, num_recommendations=200)\n",
    "        submission.append({\n",
    "            'User': original_user_id,\n",
    "            'Events': ' '.join(map(str, recommendations))\n",
    "        })\n",
    "\n",
    "    submission_df = pd.DataFrame(submission)\n",
    "    submission_df.to_csv('submission_TensorFlow.csv', index=False)\n",
    "    print(\"Submission file created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afd89159",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.41 GiB for an array with shape (103, 3137972) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32836/526505956.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0musers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'users.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'birthyear'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'object'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'timezone'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'float64'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0muser_friends\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'user_friends.csv.gz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mevents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'events.csv.gz'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'city'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'object'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'country'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'object'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'state'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'object'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'zip'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'object'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mevent_attendees\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'event_attendees.csv.gz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 626\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1966\u001b[0m                 \u001b[0mnew_col_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcol_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1967\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1968\u001b[1;33m             df = DataFrame(\n\u001b[0m\u001b[0;32m   1969\u001b[0m                 \u001b[0mnew_col_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1970\u001b[0m                 \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    776\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 778\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    779\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    501\u001b[0m             \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dtype\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"block\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         return create_block_manager_from_column_arrays(\n\u001b[0m\u001b[0;32m    153\u001b[0m             \u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrefs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrefs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_column_arrays\u001b[1;34m(arrays, axes, consolidate, refs)\u001b[0m\n\u001b[0;32m   2142\u001b[0m         \u001b[0mraise_construction_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2143\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2144\u001b[1;33m         \u001b[0mmgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2145\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmgr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1786\u001b[0m         \u001b[1;31m#  BlockManager objects not yet attached to a DataFrame.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1787\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1788\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1789\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1790\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_known_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   2267\u001b[0m     \u001b[0mnew_blocks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mBlock\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2268\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_blocks\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2269\u001b[1;33m         merged_blocks, _ = _merge_blocks(\n\u001b[0m\u001b[0;32m   2270\u001b[0m             \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcan_consolidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2271\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2300\u001b[0m         \u001b[0margsort\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2301\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2302\u001b[0m         \u001b[0mnew_mgr_locs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.41 GiB for an array with shape (103, 3137972) and data type int64"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data using Pandas, specifying data types\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "users = pd.read_csv('users.csv', dtype={'birthyear': 'object', 'timezone': 'float64'})\n",
    "user_friends = pd.read_csv('user_friends.csv.gz')\n",
    "events = pd.read_csv('events.csv.gz', dtype={'city': 'object', 'country': 'object', 'state': 'object', 'zip': 'object'})\n",
    "event_attendees = pd.read_csv('event_attendees.csv.gz')\n",
    "\n",
    "# Preview the data\n",
    "print(\"Train.csv:\")\n",
    "print(train.head(), \"\\n\")\n",
    "\n",
    "print(\"Test.csv:\")\n",
    "print(test.head(), \"\\n\")\n",
    "\n",
    "print(\"Users.csv:\")\n",
    "print(users.head(), \"\\n\")\n",
    "\n",
    "print(\"User_friends.csv:\")\n",
    "print(user_friends.head(), \"\\n\")\n",
    "\n",
    "print(\"Events.csv:\")\n",
    "print(events.head(), \"\\n\")\n",
    "\n",
    "print(\"Event_attendees.csv:\")\n",
    "print(event_attendees.head(), \"\\n\")\n",
    "\n",
    "# Data preprocessing\n",
    "train['timestamp'] = pd.to_datetime(train['timestamp'], errors='coerce')\n",
    "test['timestamp'] = pd.to_datetime(test['timestamp'], errors='coerce')\n",
    "users['joinedAt'] = pd.to_datetime(users['joinedAt'], errors='coerce')\n",
    "events['start_time'] = pd.to_datetime(events['start_time'], errors='coerce')\n",
    "\n",
    "# EDA Analysis\n",
    "print(\"Train Data Info\")\n",
    "print(train.info())\n",
    "\n",
    "print(\"\\nTest Data Info\")\n",
    "print(test.info())\n",
    "\n",
    "print(\"\\nUsers Data Info\")\n",
    "print(users.info())\n",
    "\n",
    "print(\"\\nUser Friends Data Info\")\n",
    "print(user_friends.info())\n",
    "\n",
    "print(\"\\nEvents Data Info\")\n",
    "print(events.info())\n",
    "\n",
    "print(\"\\nEvent Attendees Data Info\")\n",
    "print(event_attendees.info())\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nTrain Data Statistics\")\n",
    "print(train.describe())\n",
    "\n",
    "print(\"\\nTest Data Statistics\")\n",
    "print(test.describe())\n",
    "\n",
    "print(\"\\nUsers Data Statistics\")\n",
    "print(users.describe())\n",
    "\n",
    "print(\"\\nEvents Data Statistics\")\n",
    "print(events.describe())\n",
    "\n",
    "# Visualizing missing values\n",
    "def plot_missing_values(df, title):\n",
    "    missing = df.isnull().mean()\n",
    "    missing = missing[missing > 0]\n",
    "    missing.sort_values(inplace=True)\n",
    "    missing.plot.bar()\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_missing_values(train, \"Missing Values in Train Data\")\n",
    "plot_missing_values(test, \"Missing Values in Test Data\")\n",
    "plot_missing_values(users, \"Missing Values in Users Data\")\n",
    "plot_missing_values(events, \"Missing Values in Events Data\")\n",
    "\n",
    "# Distribution of interested and not_interested in train data\n",
    "train['interested'].value_counts().plot(kind='bar', title='Distribution of Interested and Not Interested')\n",
    "plt.show()\n",
    "\n",
    "# Creating user and event mappings\n",
    "user_ids = users['user_id'].unique().tolist()\n",
    "event_ids = train['event'].unique().tolist()\n",
    "\n",
    "user_to_index = {x: i for i, x in enumerate(user_ids)}\n",
    "event_to_index = {x: i for i, x in enumerate(event_ids)}\n",
    "\n",
    "train['user'] = train['user'].map(user_to_index)\n",
    "train['event'] = train['event'].map(event_to_index)\n",
    "test['user'] = test['user'].map(user_to_index)\n",
    "test['event'] = test['event'].map(event_to_index)\n",
    "\n",
    "# Checking for valid users and events in the test set\n",
    "test_users = test['user'].unique()\n",
    "test_events = test['event'].unique()\n",
    "\n",
    "valid_users = np.isin(test_users, list(user_to_index.values()))\n",
    "valid_events = np.isin(test_events, list(event_to_index.values()))\n",
    "\n",
    "print(f\"Number of users in the test set: {len(test_users)}\")\n",
    "print(f\"Number of events in the test set: {len(test_events)}\")\n",
    "print(f\"Number of users in the test set also in the training set: {np.sum(valid_users)}\")\n",
    "print(f\"Number of events in the test set also in the training set: {np.sum(valid_events)}\")\n",
    "\n",
    "# Filter the test data\n",
    "test = test[test['user'].isin(user_to_index.values()) & test['event'].isin(event_to_index.values())]\n",
    "\n",
    "# Check that the test set is not empty after filtering\n",
    "if test.empty:\n",
    "    print(\"The test set is empty after filtering. Ensure data is correct.\")\n",
    "else:\n",
    "    num_users = len(user_ids)\n",
    "    num_events = len(event_ids)\n",
    "\n",
    "    # Prepare data for the model\n",
    "    X = train[['user', 'event']].values\n",
    "    y = train['interested'].values\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create matrix factorization model\n",
    "    class RecommenderNet(tf.keras.Model):\n",
    "        def __init__(self, num_users, num_events, embedding_size=50, **kwargs):\n",
    "            super(RecommenderNet, self).__init__(**kwargs)\n",
    "            self.user_embedding = tf.keras.layers.Embedding(\n",
    "                num_users, embedding_size,\n",
    "                embeddings_initializer='he_normal',\n",
    "                embeddings_regularizer=tf.keras.regularizers.l2(1e-6)\n",
    "            )\n",
    "            self.event_embedding = tf.keras.layers.Embedding(\n",
    "                num_events, embedding_size,\n",
    "                embeddings_initializer='he_normal',\n",
    "                embeddings_regularizer=tf.keras.regularizers.l2(1e-6)\n",
    "            )\n",
    "            self.user_bias = tf.keras.layers.Embedding(num_users, 1)\n",
    "            self.event_bias = tf.keras.layers.Embedding(num_events, 1)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            user_vector = self.user_embedding(inputs[:, 0])\n",
    "            event_vector = self.event_embedding(inputs[:, 1])\n",
    "            \n",
    "            user_bias = self.user_bias(inputs[:, 0])\n",
    "            event_bias = self.event_bias(inputs[:, 1])\n",
    "            \n",
    "            dot_user_event = tf.tensordot(user_vector, event_vector, 2)\n",
    "            \n",
    "            x = dot_user_event + user_bias + event_bias\n",
    "            \n",
    "            return tf.nn.sigmoid(x)\n",
    "\n",
    "    # Model parameters\n",
    "    embedding_size = 50\n",
    "\n",
    "    model = RecommenderNet(num_users, num_events, embedding_size)\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        metrics=[tf.keras.metrics.AUC()]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        x=X_train,\n",
    "        y=y_train,\n",
    "        batch_size=64,\n",
    "        epochs=10,\n",
    "        validation_data=(X_val, y_val)\n",
    "    )\n",
    "\n",
    "    # Function for making recommendations\n",
    "    def recommend(user_id, model, num_recommendations=10):\n",
    "        if user_id not in user_to_index:\n",
    "            print(f\"User ID {user_id} not found.\")\n",
    "            return []\n",
    "            \n",
    "        user_index = user_to_index[user_id]\n",
    "        event_indices = np.arange(num_events)\n",
    "        \n",
    "        user_array = np.array([user_index] * num_events)\n",
    "        event_array = event_indices\n",
    "        \n",
    "        predictions = model.predict(np.vstack([user_array, event_array]).T).flatten()\n",
    "        \n",
    "        top_indices = predictions.argsort()[-num_recommendations:][::-1]\n",
    "        recommended_events = [event_ids[i] for i in top_indices]\n",
    "        \n",
    "        return recommended_events\n",
    "\n",
    "    # Create the submission file\n",
    "    submission = []\n",
    "\n",
    "    for user_id in test['user'].unique():\n",
    "        user_id = int(user_id)  # Convert to integer\n",
    "        original_user_id = user_ids[user_id]\n",
    "        recommendations = recommend(original_user_id, model, num_recommendations=200)\n",
    "        submission.append({\n",
    "            'User': original_user_id,\n",
    "            'Events': ' '.join(map(str, recommendations))\n",
    "        })\n",
    "\n",
    "    submission_df = pd.DataFrame(submission)\n",
    "    submission_df.to_csv('submission_TensorFlow.csv', index=False)\n",
    "    print(\"Submission file created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4e02acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train.csv:\n",
      "      user       event  invited                         timestamp  interested  \\\n",
      "0  3044012  1918771225        0  2012-10-02 15:53:05.754000+00:00           0   \n",
      "1  3044012  1502284248        0  2012-10-02 15:53:05.754000+00:00           0   \n",
      "2  3044012  2529072432        0  2012-10-02 15:53:05.754000+00:00           1   \n",
      "3  3044012  3072478280        0  2012-10-02 15:53:05.754000+00:00           0   \n",
      "4  3044012  1390707377        0  2012-10-02 15:53:05.754000+00:00           0   \n",
      "\n",
      "   not_interested  \n",
      "0               0  \n",
      "1               0  \n",
      "2               0  \n",
      "3               0  \n",
      "4               0   \n",
      "\n",
      "Test.csv:\n",
      "      user       event  invited                         timestamp\n",
      "0  1776192  2877501688        0  2012-11-30 11:39:01.230000+00:00\n",
      "1  1776192  3025444328        0  2012-11-30 11:39:01.230000+00:00\n",
      "2  1776192  4078218285        0  2012-11-30 11:39:01.230000+00:00\n",
      "3  1776192  1024025121        0  2012-11-30 11:39:01.230000+00:00\n",
      "4  1776192  2972428928        0  2012-11-30 11:39:21.985000+00:00 \n",
      "\n",
      "Users.csv:\n",
      "      user_id locale birthyear  gender                  joinedAt  \\\n",
      "0  3197468391  id_ID      1993    male  2012-10-02T06:40:55.524Z   \n",
      "1  3537982273  id_ID      1992    male  2012-09-29T18:03:12.111Z   \n",
      "2   823183725  en_US      1975    male  2012-10-06T03:14:07.149Z   \n",
      "3  1872223848  en_US      1991  female  2012-11-04T08:59:43.783Z   \n",
      "4  3429017717  id_ID      1995  female  2012-09-10T16:06:53.132Z   \n",
      "\n",
      "             location  timezone  \n",
      "0    Medan  Indonesia     480.0  \n",
      "1    Medan  Indonesia     420.0  \n",
      "2  Stratford  Ontario    -240.0  \n",
      "3        Tehran  Iran     210.0  \n",
      "4                 NaN     420.0   \n",
      "\n",
      "User_friends.csv:\n",
      "         user                                            friends\n",
      "0  3197468391  1346449342 3873244116 4226080662 1222907620 54...\n",
      "1  3537982273  1491560444 395798035 2036380346 899375619 3534...\n",
      "2   823183725  1484954627 1950387873 1652977611 4185960823 42...\n",
      "3  1872223848  83361640 723814682 557944478 1724049724 253059...\n",
      "4  3429017717  4253303705 2130310957 1838389374 3928735761 71... \n",
      "\n",
      "Events.csv:\n",
      "     event_id     user_id                start_time city state  zip country  \\\n",
      "0   684921758  3647864012  2012-10-31T00:00:00.001Z  NaN   NaN  NaN     NaN   \n",
      "1   244999119  3476440521  2012-11-03T00:00:00.001Z  NaN   NaN  NaN     NaN   \n",
      "2  3928440935   517514445  2012-11-05T00:00:00.001Z  NaN   NaN  NaN     NaN   \n",
      "3  2582345152   781585781  2012-10-30T00:00:00.001Z  NaN   NaN  NaN     NaN   \n",
      "4  1051165850  1016098580  2012-09-27T00:00:00.001Z  NaN   NaN  NaN     NaN   \n",
      "\n",
      "   lat  lng  c_1  ...  c_92  c_93  c_94  c_95  c_96  c_97  c_98  c_99  c_100  \\\n",
      "0  NaN  NaN    2  ...     0     1     0     0     0     0     0     0      0   \n",
      "1  NaN  NaN    2  ...     0     0     0     0     0     0     0     0      0   \n",
      "2  NaN  NaN    0  ...     0     0     0     0     0     0     0     0      0   \n",
      "3  NaN  NaN    1  ...     0     0     0     0     0     0     0     0      0   \n",
      "4  NaN  NaN    1  ...     0     0     0     0     0     0     0     0      0   \n",
      "\n",
      "   c_other  \n",
      "0        9  \n",
      "1        7  \n",
      "2       12  \n",
      "3        8  \n",
      "4        9  \n",
      "\n",
      "[5 rows x 110 columns] \n",
      "\n",
      "Event_attendees.csv:\n",
      "        event                                                yes  \\\n",
      "0  1159822043  1975964455 252302513 4226086795 3805886383 142...   \n",
      "1   686467261  2394228942 2686116898 1056558062 3792942231 41...   \n",
      "2  1186208412                                                NaN   \n",
      "3  2621578336                                                NaN   \n",
      "4   855842686  2406118796 3550897984 294255260 1125817077 109...   \n",
      "\n",
      "                                               maybe  \\\n",
      "0  2733420590 517546982 1350834692 532087573 5831...   \n",
      "1  1498184352 645689144 3770076778 331335845 4239...   \n",
      "2                              3320380166 3810793697   \n",
      "3                                                NaN   \n",
      "4  2671721559 1761448345 2356975806 2666669465 10...   \n",
      "\n",
      "                                             invited                     no  \n",
      "0  1723091036 3795873583 4109144917 3560622906 31...  3575574655 1077296663  \n",
      "1  1788073374 733302094 1830571649 676508092 7081...                    NaN  \n",
      "2                               1379121209 440668682  1728988561 2950720854  \n",
      "3                                                NaN                    NaN  \n",
      "4  1518670705 880919237 2326414227 2673818347 332...             3500235232   \n",
      "\n",
      "Train Data Info\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15398 entries, 0 to 15397\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype              \n",
      "---  ------          --------------  -----              \n",
      " 0   user            15398 non-null  int64              \n",
      " 1   event           15398 non-null  int64              \n",
      " 2   invited         15398 non-null  int64              \n",
      " 3   timestamp       15361 non-null  datetime64[ns, UTC]\n",
      " 4   interested      15398 non-null  int64              \n",
      " 5   not_interested  15398 non-null  int64              \n",
      "dtypes: datetime64[ns, UTC](1), int64(5)\n",
      "memory usage: 721.9 KB\n",
      "None\n",
      "\n",
      "Test Data Info\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10237 entries, 0 to 10236\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype              \n",
      "---  ------     --------------  -----              \n",
      " 0   user       10237 non-null  int64              \n",
      " 1   event      10237 non-null  int64              \n",
      " 2   invited    10237 non-null  int64              \n",
      " 3   timestamp  10205 non-null  datetime64[ns, UTC]\n",
      "dtypes: datetime64[ns, UTC](1), int64(3)\n",
      "memory usage: 320.0 KB\n",
      "None\n",
      "\n",
      "Users Data Info\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 38209 entries, 0 to 38208\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype              \n",
      "---  ------     --------------  -----              \n",
      " 0   user_id    38209 non-null  int64              \n",
      " 1   locale     38209 non-null  object             \n",
      " 2   birthyear  36717 non-null  object             \n",
      " 3   gender     38100 non-null  object             \n",
      " 4   joinedAt   38151 non-null  datetime64[ns, UTC]\n",
      " 5   location   32744 non-null  object             \n",
      " 6   timezone   37773 non-null  float64            \n",
      "dtypes: datetime64[ns, UTC](1), float64(1), int64(1), object(4)\n",
      "memory usage: 2.0+ MB\n",
      "None\n",
      "\n",
      "User Friends Data Info\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 38202 entries, 0 to 38201\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   user     38202 non-null  int64 \n",
      " 1   friends  38063 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 597.0+ KB\n",
      "None\n",
      "\n",
      "Events Data Info\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3137972 entries, 0 to 3137971\n",
      "Columns: 110 entries, event_id to c_other\n",
      "dtypes: datetime64[ns, UTC](1), float64(2), int64(103), object(4)\n",
      "memory usage: 2.6+ GB\n",
      "None\n",
      "\n",
      "Event Attendees Data Info\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24144 entries, 0 to 24143\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   event    24144 non-null  int64 \n",
      " 1   yes      22160 non-null  object\n",
      " 2   maybe    20977 non-null  object\n",
      " 3   invited  22322 non-null  object\n",
      " 4   no       17485 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 943.2+ KB\n",
      "None\n",
      "\n",
      "Train Data Statistics\n",
      "               user         event       invited    interested  not_interested\n",
      "count  1.539800e+04  1.539800e+04  15398.000000  15398.000000    15398.000000\n",
      "mean   2.199685e+09  2.060937e+09      0.042473      0.268282        0.033381\n",
      "std    1.268887e+09  1.190660e+09      0.201672      0.443079        0.179635\n",
      "min    3.044012e+06  1.040700e+05      0.000000      0.000000        0.000000\n",
      "25%    1.071319e+09  1.057229e+09      0.000000      0.000000        0.000000\n",
      "50%    2.259555e+09  1.996503e+09      0.000000      0.000000        0.000000\n",
      "75%    3.292836e+09  3.060446e+09      0.000000      1.000000        0.000000\n",
      "max    4.293103e+09  4.294677e+09      1.000000      1.000000        1.000000\n",
      "\n",
      "Test Data Statistics\n",
      "               user         event       invited\n",
      "count  1.023700e+04  1.023700e+04  10237.000000\n",
      "mean   2.149160e+09  2.071540e+09      0.039953\n",
      "std    1.258985e+09  1.192905e+09      0.195859\n",
      "min    1.776192e+06  1.040700e+05      0.000000\n",
      "25%    1.038961e+09  1.064770e+09      0.000000\n",
      "50%    2.151294e+09  2.007279e+09      0.000000\n",
      "75%    3.269497e+09  3.050793e+09      0.000000\n",
      "max    4.289725e+09  4.294210e+09      1.000000\n",
      "\n",
      "Users Data Statistics\n",
      "            user_id      timezone\n",
      "count  3.820900e+04  37773.000000\n",
      "mean   2.150982e+09    110.161226\n",
      "std    1.242134e+09    359.604823\n",
      "min    6.110000e+03   -720.000000\n",
      "25%    1.072041e+09   -240.000000\n",
      "50%    2.153423e+09    240.000000\n",
      "75%    3.222394e+09    420.000000\n",
      "max    4.294808e+09    840.000000\n",
      "\n",
      "Events Data Statistics\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.41 GiB for an array with shape (103, 3137972) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32836/1228793887.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nEvents Data Statistics\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;31m# Visualizing missing values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdescribe\u001b[1;34m(self, percentiles, include, exclude)\u001b[0m\n\u001b[0;32m  11974\u001b[0m         \u001b[0mmax\u001b[0m            \u001b[0mNaN\u001b[0m      \u001b[1;36m3.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11975\u001b[0m         \"\"\"\n\u001b[1;32m> 11976\u001b[1;33m         return describe_ndframe(\n\u001b[0m\u001b[0;32m  11977\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11978\u001b[0m             \u001b[0minclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\methods\\describe.py\u001b[0m in \u001b[0;36mdescribe_ndframe\u001b[1;34m(obj, include, exclude, percentiles)\u001b[0m\n\u001b[0;32m     95\u001b[0m         )\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdescriber\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpercentiles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpercentiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNDFrameT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\methods\\describe.py\u001b[0m in \u001b[0;36mdescribe\u001b[1;34m(self, percentiles)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpercentiles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_select_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[0mldesc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\methods\\describe.py\u001b[0m in \u001b[0;36m_select_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[1;31m# when some numerics are found, keep only numerics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[0mdefault_include\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"datetime\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault_include\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mselect_dtypes\u001b[1;34m(self, include, exclude)\u001b[0m\n\u001b[0;32m   5089\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5091\u001b[1;33m         \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_data_subset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5092\u001b[0m         \u001b[1;31m# error: Incompatible return value type (got \"DataFrame\", expected \"Self\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5093\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor_from_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmgr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[return-value]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 604\u001b[1;33m             \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    605\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1786\u001b[0m         \u001b[1;31m#  BlockManager objects not yet attached to a DataFrame.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1787\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1788\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1789\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1790\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_known_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   2267\u001b[0m     \u001b[0mnew_blocks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mBlock\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2268\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_blocks\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2269\u001b[1;33m         merged_blocks, _ = _merge_blocks(\n\u001b[0m\u001b[0;32m   2270\u001b[0m             \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcan_consolidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2271\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2300\u001b[0m         \u001b[0margsort\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2301\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2302\u001b[0m         \u001b[0mnew_mgr_locs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.41 GiB for an array with shape (103, 3137972) and data type int64"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the chunksize\n",
    "chunksize = 1000  # Adjust this value based on your system's memory\n",
    "\n",
    "# Load data in chunks and concatenate into a DataFrame\n",
    "train = pd.read_csv('train.csv', chunksize=chunksize)\n",
    "train = pd.concat(train)\n",
    "\n",
    "test = pd.read_csv('test.csv', chunksize=chunksize)\n",
    "test = pd.concat(test)\n",
    "\n",
    "users = pd.read_csv('users.csv', dtype={'birthyear': 'object', 'timezone': 'float64'}, chunksize=chunksize)\n",
    "users = pd.concat(users)\n",
    "\n",
    "user_friends = pd.read_csv('user_friends.csv.gz', chunksize=chunksize)\n",
    "user_friends = pd.concat(user_friends)\n",
    "\n",
    "# Reading events.csv in chunks to avoid MemoryError\n",
    "events = pd.read_csv('events.csv.gz', dtype={'city': 'object', 'country': 'object', 'state': 'object', 'zip': 'object'}, chunksize=chunksize)\n",
    "events = pd.concat(events)\n",
    "\n",
    "event_attendees = pd.read_csv('event_attendees.csv.gz', chunksize=chunksize)\n",
    "event_attendees = pd.concat(event_attendees)\n",
    "\n",
    "# Preview the data (first 5 rows from each dataset)\n",
    "print(\"Train.csv:\")\n",
    "print(train.head(), \"\\n\")\n",
    "\n",
    "print(\"Test.csv:\")\n",
    "print(test.head(), \"\\n\")\n",
    "\n",
    "print(\"Users.csv:\")\n",
    "print(users.head(), \"\\n\")\n",
    "\n",
    "print(\"User_friends.csv:\")\n",
    "print(user_friends.head(), \"\\n\")\n",
    "\n",
    "print(\"Events.csv:\")\n",
    "print(events.head(), \"\\n\")\n",
    "\n",
    "print(\"Event_attendees.csv:\")\n",
    "print(event_attendees.head(), \"\\n\")\n",
    "\n",
    "# Data preprocessing\n",
    "train['timestamp'] = pd.to_datetime(train['timestamp'], errors='coerce')\n",
    "test['timestamp'] = pd.to_datetime(test['timestamp'], errors='coerce')\n",
    "users['joinedAt'] = pd.to_datetime(users['joinedAt'], errors='coerce')\n",
    "events['start_time'] = pd.to_datetime(events['start_time'], errors='coerce')\n",
    "\n",
    "# EDA Analysis\n",
    "print(\"Train Data Info\")\n",
    "print(train.info())\n",
    "\n",
    "print(\"\\nTest Data Info\")\n",
    "print(test.info())\n",
    "\n",
    "print(\"\\nUsers Data Info\")\n",
    "print(users.info())\n",
    "\n",
    "print(\"\\nUser Friends Data Info\")\n",
    "print(user_friends.info())\n",
    "\n",
    "print(\"\\nEvents Data Info\")\n",
    "print(events.info())\n",
    "\n",
    "print(\"\\nEvent Attendees Data Info\")\n",
    "print(event_attendees.info())\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nTrain Data Statistics\")\n",
    "print(train.describe())\n",
    "\n",
    "print(\"\\nTest Data Statistics\")\n",
    "print(test.describe())\n",
    "\n",
    "print(\"\\nUsers Data Statistics\")\n",
    "print(users.describe())\n",
    "\n",
    "print(\"\\nEvents Data Statistics\")\n",
    "print(events.describe())\n",
    "\n",
    "# Visualizing missing values\n",
    "def plot_missing_values(df, title):\n",
    "    missing = df.isnull().mean()\n",
    "    missing = missing[missing > 0]\n",
    "    missing.sort_values(inplace=True)\n",
    "    missing.plot.bar()\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_missing_values(train, \"Missing Values in Train Data\")\n",
    "plot_missing_values(test, \"Missing Values in Test Data\")\n",
    "plot_missing_values(users, \"Missing Values in Users Data\")\n",
    "plot_missing_values(events, \"Missing Values in Events Data\")\n",
    "\n",
    "# Distribution of interested and not_interested in train data\n",
    "train['interested'].value_counts().plot(kind='bar', title='Distribution of Interested and Not Interested')\n",
    "plt.show()\n",
    "\n",
    "# Creating user and event mappings\n",
    "user_ids = users['user_id'].unique().tolist()\n",
    "event_ids = train['event'].unique().tolist()\n",
    "\n",
    "user_to_index = {x: i for i, x in enumerate(user_ids)}\n",
    "event_to_index = {x: i for i, x in enumerate(event_ids)}\n",
    "\n",
    "train['user'] = train['user'].map(user_to_index)\n",
    "train['event'] = train['event'].map(event_to_index)\n",
    "test['user'] = test['user'].map(user_to_index)\n",
    "test['event'] = test['event'].map(event_to_index)\n",
    "\n",
    "# Checking for valid users and events in the test set\n",
    "test_users = test['user'].unique()\n",
    "test_events = test['event'].unique()\n",
    "\n",
    "valid_users = np.isin(test_users, list(user_to_index.values()))\n",
    "valid_events = np.isin(test_events, list(event_to_index.values()))\n",
    "\n",
    "print(f\"Number of users in the test set: {len(test_users)}\")\n",
    "print(f\"Number of events in the test set: {len(test_events)}\")\n",
    "print(f\"Number of users in the test set also in the training set: {np.sum(valid_users)}\")\n",
    "print(f\"Number of events in the test set also in the training set: {np.sum(valid_events)}\")\n",
    "\n",
    "# Filter the test data\n",
    "test = test[test['user'].isin(user_to_index.values()) & test['event'].isin(event_to_index.values())]\n",
    "\n",
    "# Check that the test set is not empty after filtering\n",
    "if test.empty:\n",
    "    print(\"The test set is empty after filtering. Ensure data is correct.\")\n",
    "else:\n",
    "    num_users = len(user_ids)\n",
    "    num_events = len(event_ids)\n",
    "\n",
    "    # Prepare data for the model\n",
    "    X = train[['user', 'event']].values\n",
    "    y = train['interested'].values\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create matrix factorization model\n",
    "    class RecommenderNet(tf.keras.Model):\n",
    "        def __init__(self, num_users, num_events, embedding_size=50, **kwargs):\n",
    "            super(RecommenderNet, self).__init__(**kwargs)\n",
    "            self.user_embedding = tf.keras.layers.Embedding(\n",
    "                num_users, embedding_size,\n",
    "                embeddings_initializer='he_normal',\n",
    "                embeddings_regularizer=tf.keras.regularizers.l2(1e-6)\n",
    "            )\n",
    "            self.event_embedding = tf.keras.layers.Embedding(\n",
    "                num_events, embedding_size,\n",
    "                embeddings_initializer='he_normal',\n",
    "                embeddings_regularizer=tf.keras.regularizers.l2(1e-6)\n",
    "            )\n",
    "            self.user_bias = tf.keras.layers.Embedding(num_users, 1)\n",
    "            self.event_bias = tf.keras.layers.Embedding(num_events, 1)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            user_vector = self.user_embedding(inputs[:, 0])\n",
    "            event_vector = self.event_embedding(inputs[:, 1])\n",
    "            \n",
    "            user_bias = self.user_bias(inputs[:, 0])\n",
    "            event_bias = self.event_bias(inputs[:, 1])\n",
    "            \n",
    "            dot_user_event = tf.tensordot(user_vector, event_vector, 2)\n",
    "            \n",
    "            x = dot_user_event + user_bias + event_bias\n",
    "            \n",
    "            return tf.nn.sigmoid(x)\n",
    "\n",
    "    # Model parameters\n",
    "    embedding_size = 50\n",
    "\n",
    "    model = RecommenderNet(num_users, num_events, embedding_size)\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        metrics=[tf.keras.metrics.AUC()]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        x=X_train,\n",
    "        y=y_train,\n",
    "        batch_size=64,\n",
    "        epochs=10,\n",
    "        validation_data=(X_val, y_val)\n",
    "    )\n",
    "\n",
    "    # Function for making recommendations\n",
    "    def recommend(user_id, model, num_recommendations=10):\n",
    "        if user_id not in user_to_index:\n",
    "            print(f\"User ID {user_id} not found.\")\n",
    "            return []\n",
    "            \n",
    "        user_index = user_to_index[user_id]\n",
    "        event_indices = np.arange(num_events)\n",
    "        \n",
    "        user_array = np.array([user_index] * num_events)\n",
    "        event_array = event_indices\n",
    "        \n",
    "        predictions = model.predict(np.vstack([user_array, event_array]).T).flatten()\n",
    "        \n",
    "        top_indices = predictions.argsort()[-num_recommendations:][::-1]\n",
    "        recommended_events = [event_ids[i] for i in top_indices]\n",
    "        \n",
    "        return recommended_events\n",
    "\n",
    "    # Create the submission file\n",
    "    submission = []\n",
    "\n",
    "    for user_id in test['user'].unique():\n",
    "        user_id = int(user_id)  # Convert to integer\n",
    "        original_user_id = user_ids[user_id]\n",
    "        recommendations = recommend(original_user_id, model, num_recommendations=200)\n",
    "        submission.append({\n",
    "            'User': original_user_id,\n",
    "            'Events': ' '.join(map(str, recommendations))\n",
    "        })\n",
    "\n",
    "    submission_df = pd.DataFrame(submission)\n",
    "    submission_df.to_csv('submission_TensorFlow.csv', index=False)\n",
    "    print(\"Submission file created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fd4ebc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32836/3345664121.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m# Load and process user_friends.csv.gz in chunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'user_friends.csv.gz'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0muser_friends_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1841\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1843\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1844\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1845\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mget_chunk\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1983\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1984\u001b[0m             \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_currow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1985\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1987\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mSelf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m                 \u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m  \u001b[1;31m# type: ignore[attr-defined]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1925\u001b[0m                 )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m                 \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m                 \u001b[1;31m# destructive to chunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: out of memory"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the chunksize to control memory usage\n",
    "chunksize = 100000  # Adjust this based on your available memory\n",
    "\n",
    "# Initialize lists to store processed chunks if you want to accumulate the results\n",
    "train_data = []\n",
    "test_data = []\n",
    "users_data = []\n",
    "user_friends_data = []\n",
    "events_data = []\n",
    "event_attendees_data = []\n",
    "\n",
    "# Load and process train.csv in chunks\n",
    "for chunk in pd.read_csv('train.csv', chunksize=chunksize):\n",
    "    chunk['timestamp'] = pd.to_datetime(chunk['timestamp'], errors='coerce')\n",
    "    train_data.append(chunk)\n",
    "\n",
    "# Load and process test.csv in chunks\n",
    "for chunk in pd.read_csv('test.csv', chunksize=chunksize):\n",
    "    chunk['timestamp'] = pd.to_datetime(chunk['timestamp'], errors='coerce')\n",
    "    test_data.append(chunk)\n",
    "\n",
    "# Load and process users.csv in chunks\n",
    "for chunk in pd.read_csv('users.csv', dtype={'birthyear': 'object', 'timezone': 'float64'}, chunksize=chunksize):\n",
    "    chunk['joinedAt'] = pd.to_datetime(chunk['joinedAt'], errors='coerce')\n",
    "    users_data.append(chunk)\n",
    "\n",
    "# Load and process user_friends.csv.gz in chunks\n",
    "for chunk in pd.read_csv('user_friends.csv.gz', chunksize=chunksize):\n",
    "    user_friends_data.append(chunk)\n",
    "\n",
    "# Load and process events.csv.gz in chunks\n",
    "for chunk in pd.read_csv('events.csv.gz', dtype={'city': 'object', 'country': 'object', 'state': 'object', 'zip': 'object'}, chunksize=chunksize):\n",
    "    chunk['start_time'] = pd.to_datetime(chunk['start_time'], errors='coerce')\n",
    "    events_data.append(chunk)\n",
    "\n",
    "# Load and process event_attendees.csv.gz in chunks\n",
    "for chunk in pd.read_csv('event_attendees.csv.gz', chunksize=chunksize):\n",
    "    event_attendees_data.append(chunk)\n",
    "\n",
    "# Combine the chunks into a single DataFrame (if needed)\n",
    "train_df = pd.concat(train_data, ignore_index=True)\n",
    "test_df = pd.concat(test_data, ignore_index=True)\n",
    "users_df = pd.concat(users_data, ignore_index=True)\n",
    "user_friends_df = pd.concat(user_friends_data, ignore_index=True)\n",
    "events_df = pd.concat(events_data, ignore_index=True)\n",
    "event_attendees_df = pd.concat(event_attendees_data, ignore_index=True)\n",
    "\n",
    "# Preview the data\n",
    "print(\"Train.csv:\")\n",
    "print(train_df.head(), \"\\n\")\n",
    "\n",
    "print(\"Test.csv:\")\n",
    "print(test_df.head(), \"\\n\")\n",
    "\n",
    "print(\"Users.csv:\")\n",
    "print(users_df.head(), \"\\n\")\n",
    "\n",
    "print(\"User_friends.csv:\")\n",
    "print(user_friends_df.head(), \"\\n\")\n",
    "\n",
    "print(\"Events.csv:\")\n",
    "print(events_df.head(), \"\\n\")\n",
    "\n",
    "print(\"Event_attendees.csv:\")\n",
    "print(event_attendees_df.head(), \"\\n\")\n",
    "\n",
    "# Basic statistics and analysis\n",
    "print(\"\\nTrain Data Statistics\")\n",
    "print(train_df.describe())\n",
    "\n",
    "print(\"\\nTest Data Statistics\")\n",
    "print(test_df.describe())\n",
    "\n",
    "print(\"\\nUsers Data Statistics\")\n",
    "print(users_df.describe())\n",
    "\n",
    "print(\"\\nEvents Data Statistics\")\n",
    "print(events_df.describe())\n",
    "\n",
    "# Visualizing missing values\n",
    "def plot_missing_values(df, title):\n",
    "    missing = df.isnull().mean()\n",
    "    missing = missing[missing > 0]\n",
    "    missing.sort_values(inplace=True)\n",
    "    missing.plot.bar()\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_missing_values(train_df, \"Missing Values in Train Data\")\n",
    "plot_missing_values(test_df, \"Missing Values in Test Data\")\n",
    "plot_missing_values(users_df, \"Missing Values in Users Data\")\n",
    "plot_missing_values(events_df, \"Missing Values in Events Data\")\n",
    "\n",
    "# Distribution of interested and not_interested in train data\n",
    "train_df['interested'].value_counts().plot(kind='bar', title='Distribution of Interested and Not Interested')\n",
    "plt.show()\n",
    "\n",
    "# Creating user and event mappings\n",
    "user_ids = users_df['user_id'].unique().tolist()\n",
    "event_ids = train_df['event'].unique().tolist()\n",
    "\n",
    "user_to_index = {x: i for i, x in enumerate(user_ids)}\n",
    "event_to_index = {x: i for i, x in enumerate(event_ids)}\n",
    "\n",
    "train_df['user'] = train_df['user'].map(user_to_index)\n",
    "train_df['event'] = train_df['event'].map(event_to_index)\n",
    "test_df['user'] = test_df['user'].map(user_to_index)\n",
    "test_df['event'] = test_df['event'].map(event_to_index)\n",
    "\n",
    "# Checking for valid users and events in the test set\n",
    "test_users = test_df['user'].unique()\n",
    "test_events = test_df['event'].unique()\n",
    "\n",
    "valid_users = np.isin(test_users, list(user_to_index.values()))\n",
    "valid_events = np.isin(test_events, list(event_to_index.values()))\n",
    "\n",
    "print(f\"Number of users in the test set: {len(test_users)}\")\n",
    "print(f\"Number of events in the test set: {len(test_events)}\")\n",
    "print(f\"Number of users in the test set also in the training set: {np.sum(valid_users)}\")\n",
    "print(f\"Number of events in the test set also in the training set: {np.sum(valid_events)}\")\n",
    "\n",
    "# Filter the test data\n",
    "test_df = test_df[test_df['user'].isin(user_to_index.values()) & test_df['event'].isin(event_to_index.values())]\n",
    "\n",
    "# Check that the test set is not empty after filtering\n",
    "if test_df.empty:\n",
    "    print(\"The test set is empty after filtering. Ensure data is correct.\")\n",
    "else:\n",
    "    num_users = len(user_ids)\n",
    "    num_events = len(event_ids)\n",
    "\n",
    "    # Prepare data for the model\n",
    "    X = train_df[['user', 'event']].values\n",
    "    y = train_df['interested'].values\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create matrix factorization model\n",
    "    class RecommenderNet(tf.keras.Model):\n",
    "        def __init__(self, num_users, num_events, embedding_size=50, **kwargs):\n",
    "            super(RecommenderNet, self).__init__(**kwargs)\n",
    "            self.user_embedding = tf.keras.layers.Embedding(\n",
    "                num_users, embedding_size,\n",
    "                embeddings_initializer='he_normal',\n",
    "                embeddings_regularizer=tf.keras.regularizers.l2(1e-6)\n",
    "            )\n",
    "            self.event_embedding = tf.keras.layers.Embedding(\n",
    "                num_events, embedding_size,\n",
    "                embeddings_initializer='he_normal',\n",
    "                embeddings_regularizer=tf.keras.regularizers.l2(1e-6)\n",
    "            )\n",
    "            self.user_bias = tf.keras.layers.Embedding(num_users, 1)\n",
    "            self.event_bias = tf.keras.layers.Embedding(num_events, 1)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            user_vector = self.user_embedding(inputs[:, 0])\n",
    "            event_vector = self.event_embedding(inputs[:, 1])\n",
    "            \n",
    "            user_bias = self.user_bias(inputs[:, 0])\n",
    "            event_bias = self.event_bias(inputs[:, 1])\n",
    "            \n",
    "            dot_user_event = tf.tensordot(user_vector, event_vector, 2)\n",
    "            \n",
    "            x = dot_user_event + user_bias + event_bias\n",
    "            \n",
    "            return tf.nn.sigmoid(x)\n",
    "\n",
    "    # Model parameters\n",
    "    embedding_size = 50\n",
    "\n",
    "    model = RecommenderNet(num_users, num_events, embedding_size)\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        metrics=[tf.keras.metrics.AUC()]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        x=X_train,\n",
    "        y=y_train,\n",
    "        batch_size=64,\n",
    "        epochs=10,\n",
    "        validation_data=(X_val, y_val)\n",
    "    )\n",
    "\n",
    "    # Function for making recommendations\n",
    "    def recommend(user_id, model, num_recommendations=10):\n",
    "        if user_id not in user_to_index:\n",
    "            print(f\"User ID {user_id} not found.\")\n",
    "            return []\n",
    "            \n",
    "        user_index = user_to_index[user_id]\n",
    "        event_indices = np.arange(num_events)\n",
    "        \n",
    "        user_array = np.array([user_index] * num_events)\n",
    "        event_array = event_indices\n",
    "        \n",
    "        predictions = model.predict(np.vstack([user_array, event_array]).T).flatten()\n",
    "        \n",
    "        top_indices = predictions.argsort()[-num_recommendations:][::-1]\n",
    "        recommended_events = [event_ids[i] for i in top_indices]\n",
    "        \n",
    "        return recommended_events\n",
    "\n",
    "    # Create the submission file\n",
    "    submission = []\n",
    "\n",
    "    for user_id in test_df['user'].unique():\n",
    "        user_id = int(user_id)  # Convert to integer\n",
    "        original_user_id = user_ids[user_id]\n",
    "        recommendations = recommend(original_user_id, model, num_recommendations=200)\n",
    "        submission.append({\n",
    "            'User': original_user_id,\n",
    "            'Events': ' '.join(map(str, recommendations))\n",
    "        })\n",
    "\n",
    "    submission_df = pd.DataFrame(submission)\n",
    "    submission_df.to_csv('submission_TensorFlow.csv', index=False)\n",
    "    print(\"Submission file created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac7f2898",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32836/3833965055.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'user_friends.csv.gz'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Skip header\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0muser_friends_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;31m# decode input (taking the buffer into account)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m         \u001b[1;31m# keep undecoded input until the next call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import gzip\n",
    "\n",
    "# Define the chunksize to control memory usage\n",
    "chunksize = 50000  # Reduced chunksize\n",
    "\n",
    "# Initialize lists to store processed chunks if you want to accumulate the results\n",
    "train_data = []\n",
    "test_data = []\n",
    "users_data = []\n",
    "user_friends_data = []\n",
    "events_data = []\n",
    "event_attendees_data = []\n",
    "\n",
    "# Load and process train.csv in chunks\n",
    "for chunk in pd.read_csv('train.csv', chunksize=chunksize, iterator=True):\n",
    "    chunk['timestamp'] = pd.to_datetime(chunk['timestamp'], errors='coerce')\n",
    "    train_data.append(chunk)\n",
    "\n",
    "# Load and process test.csv in chunks\n",
    "for chunk in pd.read_csv('test.csv', chunksize=chunksize, iterator=True):\n",
    "    chunk['timestamp'] = pd.to_datetime(chunk['timestamp'], errors='coerce')\n",
    "    test_data.append(chunk)\n",
    "\n",
    "# Load and process users.csv in chunks\n",
    "for chunk in pd.read_csv('users.csv', dtype={'birthyear': 'object', 'timezone': 'float64'}, chunksize=chunksize, iterator=True):\n",
    "    chunk['joinedAt'] = pd.to_datetime(chunk['joinedAt'], errors='coerce')\n",
    "    users_data.append(chunk)\n",
    "\n",
    "# Load and process user_friends.csv.gz using gzip\n",
    "with gzip.open('user_friends.csv.gz', mode='rt', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    header = next(reader)  # Skip header\n",
    "    for row in reader:\n",
    "        user_friends_data.append(row)\n",
    "\n",
    "# Load and process events.csv.gz in chunks\n",
    "for chunk in pd.read_csv('events.csv.gz', dtype={'city': 'object', 'country': 'object', 'state': 'object', 'zip': 'object'}, chunksize=chunksize, iterator=True):\n",
    "    chunk['start_time'] = pd.to_datetime(chunk['start_time'], errors='coerce')\n",
    "    events_data.append(chunk)\n",
    "\n",
    "# Load and process event_attendees.csv.gz in chunks\n",
    "for chunk in pd.read_csv('event_attendees.csv.gz', chunksize=chunksize, iterator=True):\n",
    "    event_attendees_data.append(chunk)\n",
    "\n",
    "# Combine the chunks into a single DataFrame (if needed)\n",
    "train_df = pd.concat(train_data, ignore_index=True)\n",
    "test_df = pd.concat(test_data, ignore_index=True)\n",
    "users_df = pd.concat(users_data, ignore_index=True)\n",
    "events_df = pd.concat(events_data, ignore_index=True)\n",
    "event_attendees_df = pd.concat(event_attendees_data, ignore_index=True)\n",
    "\n",
    "# Process the user_friends_data manually if necessary, as it's handled by csv reader\n",
    "user_friends_df = pd.DataFrame(user_friends_data, columns=header)\n",
    "\n",
    "# Preview the data\n",
    "print(\"Train.csv:\")\n",
    "print(train_df.head(), \"\\n\")\n",
    "\n",
    "print(\"Test.csv:\")\n",
    "print(test_df.head(), \"\\n\")\n",
    "\n",
    "print(\"Users.csv:\")\n",
    "print(users_df.head(), \"\\n\")\n",
    "\n",
    "print(\"User_friends.csv:\")\n",
    "print(user_friends_df.head(), \"\\n\")\n",
    "\n",
    "print(\"Events.csv:\")\n",
    "print(events_df.head(), \"\\n\")\n",
    "\n",
    "print(\"Event_attendees.csv:\")\n",
    "print(event_attendees_df.head(), \"\\n\")\n",
    "\n",
    "# The rest of your code for data processing, model training, etc. goes here...\n",
    "\n",
    "\n",
    "# The rest of your code for data processing, model training, etc. goes here...\n",
    "# Visualizing missing values\n",
    "def plot_missing_values(df, title):\n",
    "    missing = df.isnull().mean()\n",
    "    missing = missing[missing > 0]\n",
    "    missing.sort_values(inplace=True)\n",
    "    missing.plot.bar()\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_missing_values(train_df, \"Missing Values in Train Data\")\n",
    "plot_missing_values(test_df, \"Missing Values in Test Data\")\n",
    "plot_missing_values(users_df, \"Missing Values in Users Data\")\n",
    "plot_missing_values(events_df, \"Missing Values in Events Data\")\n",
    "\n",
    "# Distribution of interested and not_interested in train data\n",
    "train_df['interested'].value_counts().plot(kind='bar', title='Distribution of Interested and Not Interested')\n",
    "plt.show()\n",
    "\n",
    "# Creating user and event mappings\n",
    "user_ids = users_df['user_id'].unique().tolist()\n",
    "event_ids = train_df['event'].unique().tolist()\n",
    "\n",
    "user_to_index = {x: i for i, x in enumerate(user_ids)}\n",
    "event_to_index = {x: i for i, x in enumerate(event_ids)}\n",
    "\n",
    "train_df['user'] = train_df['user'].map(user_to_index)\n",
    "train_df['event'] = train_df['event'].map(event_to_index)\n",
    "test_df['user'] = test_df['user'].map(user_to_index)\n",
    "test_df['event'] = test_df['event'].map(event_to_index)\n",
    "\n",
    "# Checking for valid users and events in the test set\n",
    "test_users = test_df['user'].unique()\n",
    "test_events = test_df['event'].unique()\n",
    "\n",
    "valid_users = np.isin(test_users, list(user_to_index.values()))\n",
    "valid_events = np.isin(test_events, list(event_to_index.values()))\n",
    "\n",
    "print(f\"Number of users in the test set: {len(test_users)}\")\n",
    "print(f\"Number of events in the test set: {len(test_events)}\")\n",
    "print(f\"Number of users in the test set also in the training set: {np.sum(valid_users)}\")\n",
    "print(f\"Number of events in the test set also in the training set: {np.sum(valid_events)}\")\n",
    "\n",
    "# Filter the test data\n",
    "test_df = test_df[test_df['user'].isin(user_to_index.values()) & test_df['event'].isin(event_to_index.values())]\n",
    "\n",
    "# Check that the test set is not empty after filtering\n",
    "if test_df.empty:\n",
    "    print(\"The test set is empty after filtering. Ensure data is correct.\")\n",
    "else:\n",
    "    num_users = len(user_ids)\n",
    "    num_events = len(event_ids)\n",
    "\n",
    "    # Prepare data for the model\n",
    "    X = train_df[['user', 'event']].values\n",
    "    y = train_df['interested'].values\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create matrix factorization model\n",
    "    class RecommenderNet(tf.keras.Model):\n",
    "        def __init__(self, num_users, num_events, embedding_size=50, **kwargs):\n",
    "            super(RecommenderNet, self).__init__(**kwargs)\n",
    "            self.user_embedding = tf.keras.layers.Embedding(\n",
    "                num_users, embedding_size,\n",
    "                embeddings_initializer='he_normal',\n",
    "                embeddings_regularizer=tf.keras.regularizers.l2(1e-6)\n",
    "            )\n",
    "            self.event_embedding = tf.keras.layers.Embedding(\n",
    "                num_events, embedding_size,\n",
    "                embeddings_initializer='he_normal',\n",
    "                embeddings_regularizer=tf.keras.regularizers.l2(1e-6)\n",
    "            )\n",
    "            self.user_bias = tf.keras.layers.Embedding(num_users, 1)\n",
    "            self.event_bias = tf.keras.layers.Embedding(num_events, 1)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            user_vector = self.user_embedding(inputs[:, 0])\n",
    "            event_vector = self.event_embedding(inputs[:, 1])\n",
    "            \n",
    "            user_bias = self.user_bias(inputs[:, 0])\n",
    "            event_bias = self.event_bias(inputs[:, 1])\n",
    "            \n",
    "            dot_user_event = tf.tensordot(user_vector, event_vector, 2)\n",
    "            \n",
    "            x = dot_user_event + user_bias + event_bias\n",
    "            \n",
    "            return tf.nn.sigmoid(x)\n",
    "\n",
    "    # Model parameters\n",
    "    embedding_size = 50\n",
    "\n",
    "    model = RecommenderNet(num_users, num_events, embedding_size)\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        metrics=[tf.keras.metrics.AUC()]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        x=X_train,\n",
    "        y=y_train,\n",
    "        batch_size=64,\n",
    "        epochs=10,\n",
    "        validation_data=(X_val, y_val)\n",
    "    )\n",
    "\n",
    "    # Function for making recommendations\n",
    "    def recommend(user_id, model, num_recommendations=10):\n",
    "        if user_id not in user_to_index:\n",
    "            print(f\"User ID {user_id} not found.\")\n",
    "            return []\n",
    "            \n",
    "        user_index = user_to_index[user_id]\n",
    "        event_indices = np.arange(num_events)\n",
    "        \n",
    "        user_array = np.array([user_index] * num_events)\n",
    "        event_array = event_indices\n",
    "        \n",
    "        predictions = model.predict(np.vstack([user_array, event_array]).T).flatten()\n",
    "        \n",
    "        top_indices = predictions.argsort()[-num_recommendations:][::-1]\n",
    "        recommended_events = [event_ids[i] for i in top_indices]\n",
    "        \n",
    "        return recommended_events\n",
    "\n",
    "    # Create the submission file\n",
    "    submission = []\n",
    "\n",
    "    for user_id in test_df['user'].unique():\n",
    "        user_id = int(user_id)  # Convert to integer\n",
    "        original_user_id = user_ids[user_id]\n",
    "        recommendations = recommend(original_user_id, model, num_recommendations=200)\n",
    "        submission.append({\n",
    "            'User': original_user_id,\n",
    "            'Events': ' '.join(map(str, recommendations))\n",
    "        })\n",
    "\n",
    "    submission_df = pd.DataFrame(submission)\n",
    "    submission_df.to_csv('submission_TensorFlow.csv', index=False)\n",
    "    print(\"Submission file created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977597e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
